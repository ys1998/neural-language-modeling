# Neural Language Modeling
This repository is basically a collection of my work related to language modeling using neural networks.
It contains discussions and Python3/TensorFlow implementations of several standard research papers related to the field, along with a list and brief description of references.

## Research papers
* **Recurrent Neural Network based Language Model**, Mikolov et al, 2010 \[ [pdf](References/mikolov.pdf) | [discussion](mikolov.ipynb) ]
* **Recurrent Neural Network Regularization**, Zaremba et al, 2014 \[ [pdf](References/zaremba.pdf) | [discussion](zaremba.ipynb) ]
## References
* [**Vector Representations of Words** - TensorFlow](https://www.tensorflow.org/tutorials/word2vec) : This tutorial is about the `word2vec` model described by Mikolov et al \[[pdf](References/word_embeddings_mikolov.pdf)] and its implementation using TensorFlow
* [**Deep Learning for NLP** - CS224D, Stanford University](https://cs224d.stanford.edu/lecture_notes/) : These notes talk in sufficient details about the various methods for learning word embeddings (CBOW, Skip-gram, Negative sampling) as well as neural network models suitable for NLP. 
